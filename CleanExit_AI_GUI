import sys

from PyQt5.QtWidgets import (
    QApplication, QWidget, QLabel, QPushButton, QVBoxLayout,
    QFileDialog, QMessageBox
)
from PyQt5.QtGui import QPixmap, QImage
from PyQt5.QtCore import Qt
import cv2
import numpy as np
import torch
import torch.nn as nn
from torchvision import models,transforms
from pathlib import Path

class CleanExit_AI(QWidget):
    """
    ExitGuard - ì±…ìƒ ì²­ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” PyQt GUI

    ê¸°ëŠ¥:
    1. ì´ë¯¸ì§€ ì—…ë¡œë“œ
    2. 1ë‹¨ê³„ ë¶„ë¥˜ ëª¨ë¸ (ì˜ˆ: MobileNetV3)ë¡œ clean / messy íŒë³„
    3. messyì¼ ê²½ìš° 2ë‹¨ê³„ YOLOë¡œ ë¬¼ì²´ íƒì§€ í›„ bounding box ê·¸ë ¤ì„œ í‘œì‹œ
    """

    def __init__(self):
        super().__init__()

        # ê¸°ë³¸ ìœˆë„ìš° ì„¤ì •
        self.setWindowTitle("CleanExit_AI - Desk Cleanliness Inspector")
        self.setGeometry(300, 200, 500, 650)

        # ì´ë¯¸ì§€ í‘œì‹œ ì˜ì—­
        self.image_label = QLabel("ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”")
        self.image_label.setAlignment(Qt.AlignCenter)
        self.image_label.setFixedSize(450, 450)

        # ë²„íŠ¼: ì‚¬ì§„ ì—…ë¡œë“œ
        self.btn_upload = QPushButton("ì‚¬ì§„ ì—…ë¡œë“œ")
        self.btn_upload.clicked.connect(self.load_image)

        # ë²„íŠ¼: íŒë‹¨í•˜ê¸° (ë¶„ë¥˜ â†’ í•„ìš” ì‹œ íƒì§€)
        self.btn_predict = QPushButton("íŒë‹¨í•˜ê¸°")
        self.btn_predict.clicked.connect(self.run_classifier)

        # ë ˆì´ì•„ì›ƒ ì„¤ì •
        layout = QVBoxLayout()
        layout.addWidget(self.image_label)
        layout.addWidget(self.btn_upload)
        layout.addWidget(self.btn_predict)
        self.setLayout(layout)

        # í˜„ì¬ ì„ íƒëœ ì´ë¯¸ì§€ ê²½ë¡œ ì €ì¥
        self.current_image_path = None


        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ ê²½ë¡œ ê³„ì‚°(../CleanExit_AI)
        self.base_dir = Path(__file__).resolve().parents[0]

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu" )

        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (MobileNet ì…ë ¥ìš©)
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224,224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225],
            ),
        ])

        self.class_names = ["clean", "messy"]

        #  ëª¨ë¸ ë¡œë” (ì§€ê¸ˆì€ placeholder, ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì½”ë“œë¡œ êµì²´ ì˜ˆì •)
        self.classifier_model = self.load_classifier_model()
        self.yolo_model = self.load_yolo_model()

    # -----------------------------
    #  ëª¨ë¸ ë¡œë”©
    # -----------------------------
    def load_classifier_model(self):
        """
        1ë‹¨ê³„ ë¶„ë¥˜ ëª¨ë¸(MobileNetV3 ë“±)ì„ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜.
        checkpoints/mobilenet_baseline.pth ì‚¬ìš©
        """
        try:
            ckpt_path = self.base_dir / "checkpoints" / "mobilenet_baseline.pth"
            print(f"ë¶„ë¥˜ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸: {ckpt_path}")

            # í•™ìŠµì‹œí‚¬ë•Œ ì‚¬ìš©í•œ êµ¬ì¡°ì™€ ë™ì¼í•œ ëª¨ë¸ êµ¬ì¡°
            nc = 2
            model = models.mobilenet_v3_large(
                weights = models.MobileNet_V3_Large_Weights.IMAGENET1K_V1
            )
            model.classifier[3] = nn.Linear(model.classifier[3].in_features, nc)

            state_dict = torch.load(ckpt_path, map_location='cpu')
            model.load_state_dict(state_dict)

            model.to(self.device)
            model.eval()

            print("MobileNetV3-Large classifier model loaded!")
            return model

        except Exception as e:
            print(f"MobileNetV3-Large ë¡œë”© ì‹¤íŒ¨: {e}")
            return None


    def load_yolo_model(self):
        """
        2ë‹¨ê³„ ê°ì²´ íƒì§€ ëª¨ë¸(YOLO ë“±)ì„ ë¡œë”©í•˜ëŠ” í•¨ìˆ˜.
        ì§€ê¸ˆì€ placeholderë¡œ Noneì„ ë¦¬í„´í•˜ê³ , ì¶”í›„ ì‹¤ì œ ëª¨ë¸ ë¡œë”© ì½”ë“œë¡œ êµì²´.
        """
        print("YOLO object detection model loaded (placeholder)")
        return None

    # -----------------------------
    # ğŸ¯ ì´ë¯¸ì§€ ì—…ë¡œë“œ
    # -----------------------------
    def load_image(self):
        """
        íŒŒì¼ ì„ íƒ ì°½ì„ ë„ì›Œì„œ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ê³ ,
        QLabel ì˜ì—­ì— ì„ íƒí•œ ì´ë¯¸ì§€ë¥¼ ë¯¸ë¦¬ë³´ê¸°ë¡œ ë„ì›€.
        """
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "ì´ë¯¸ì§€ ì„ íƒ",
            "",
            "Image Files (*.png *.jpg *.jpeg)"
        )

        if file_path:
            self.current_image_path = file_path
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •í•´ì„œ ë¼ë²¨ì— í‘œì‹œ
            pixmap = QPixmap(file_path).scaled(
                450, 450, Qt.KeepAspectRatio, Qt.SmoothTransformation
            )
            self.image_label.setPixmap(pixmap)

    # -----------------------------
    #  1ë‹¨ê³„: MobileNet ë¶„ë¥˜
    # -----------------------------
    def classify_image(self, img_path: str) -> str:
        """
        1ë‹¨ê³„ ë¶„ë¥˜ ëª¨ë¸ë¡œ ì´ë¯¸ì§€ì˜ ì²­ê²° ìƒíƒœë¥¼ ì˜ˆì¸¡.
        return ê°’ì€ "clean" ë˜ëŠ” "messy"

        TODO:
            - img_pathë¡œ ì´ë¯¸ì§€ë¥¼ ì—´ì–´ì„œ ì „ì²˜ë¦¬
            - self.classifier_modelë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
            - ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¦¬í„´
        """
        if self.classifier_model is None:
            QMessageBox.warning(self, "ëª¨ë¸ ì˜¤ë¥˜", "ë¶„ë¥˜ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "messy"
        img_bgr = cv2.imread(img_path)
        if img_bgr is None:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return "messy"

        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        tensor = self.transform(img_rgb).unsqueeze(0).to(self.device)

        with torch.no_grad():
            logits = self.classifier_model(tensor)
            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()

        idx = int(np.argmax(probs))
        label = self.class_names[idx]
        self.last_class_probs = probs

        print(f"[Classifier] {label} (clean={probs[0]:.2f}, messy={probs[1]:.2f})")
        return label

    # -----------------------------
    #  2ë‹¨ê³„: YOLO íƒì§€
    # -----------------------------
    def detect_objects(self, img_path: str):
        """
        2ë‹¨ê³„ íƒì§€ ëª¨ë¸(YOLO)ë¡œ ì±…ìƒ ìœ„ ë¬¼ì²´ë¥¼ íƒì§€í•˜ëŠ” í•¨ìˆ˜.

        Args:
            img_path (str): ì…ë ¥ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ

        Returns:
            list[tuple]: (x1, y1, x2, y2, label, confidence) ë¦¬ìŠ¤íŠ¸

        TODO:
            - self.yolo_model(img_path) í˜•íƒœë¡œ ì‹¤ì œ YOLO inference ìˆ˜í–‰
            - ê²°ê³¼ë¥¼ bounding box ì¢Œí‘œ / ë¼ë²¨ / confidenceë¡œ íŒŒì‹±í•˜ì—¬ ë¦¬í„´
        """
        print("Running YOLO detector ... (placeholder)")

        QMessageBox.information(
            self,
            "ì•Œë¦¼",
            "YOLO íƒì§€ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
        return []

    # -----------------------------
    # ğŸ¯ YOLO ë°•ìŠ¤ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë¦¬ê¸°
    # -----------------------------
    def draw_boxes(self, img_path: str, boxes):
        """
        YOLOì—ì„œ ë°˜í™˜ëœ bounding boxë“¤ì„ ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— ê·¸ë ¤ì„œ
        ê·¸ ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ GUIì— í‘œì‹œ.

        Args:
            img_path (str): ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ
            boxes (list): (x1, y1, x2, y2, label, confidence) ë¦¬ìŠ¤íŠ¸
        """
        img = cv2.imread(img_path)

        if img is None:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        # bounding box ê·¸ë¦¬ê¸°
        for (x1, y1, x2, y2, label, conf) in boxes:
            # ì‚¬ê°í˜•
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            # ë¼ë²¨ + confidence í…ìŠ¤íŠ¸
            cv2.putText(
                img,
                f"{label} {conf:.2f}",
                (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX,
                0.6,
                (0, 255, 0),
                2,
            )

        # OpenCV(BGR) â†’ RGB â†’ QImage â†’ QPixmap ë³€í™˜ í›„ ë¼ë²¨ì— í‘œì‹œ
        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb.shape
        qimg = QImage(rgb.data, w, h, w * ch, QImage.Format_RGB888)
        pixmap = QPixmap.fromImage(qimg).scaled(
            450, 450, Qt.KeepAspectRatio, Qt.SmoothTransformation
        )
        self.image_label.setPixmap(pixmap)

    # -----------------------------
    #  ë²„íŠ¼ í´ë¦­ â†’ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # -----------------------------
    def run_classifier(self):
        """
        'íŒë‹¨í•˜ê¸°' ë²„íŠ¼ì´ ëˆŒë ¸ì„ ë•Œ ë™ì‘í•˜ëŠ” ë©”ì¸ íŒŒì´í”„ë¼ì¸.

        1) ì´ë¯¸ì§€ ì—…ë¡œë“œ ì—¬ë¶€ í™•ì¸
        2) 1ë‹¨ê³„ ë¶„ë¥˜ ëª¨ë¸ ì‹¤í–‰ â†’ clean / messy
        3) cleanì´ë©´: íŒì—…ìœ¼ë¡œ clean ì•ˆë‚´
        4) messyì´ë©´:
            - íŒì—…ìœ¼ë¡œ messy ì•ˆë‚´
            - 2ë‹¨ê³„ YOLO íƒì§€ ì‹¤í–‰
            - bounding box ê·¸ë ¤ì„œ í™”ë©´ì— í‘œì‹œ
        """
        if not self.current_image_path:
            QMessageBox.warning(self, "ì˜¤ë¥˜", "ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return

        # 1) MobileNet ë¶„ë¥˜ ì‹¤í–‰
        result = self.classify_image(self.current_image_path)

        if result == "clean":
            QMessageBox.information(self, "íŒë‹¨ ê²°ê³¼", "âœ” CLEAN ìƒíƒœì…ë‹ˆë‹¤")
            return

        # messyì´ë©´ YOLO íƒì§€ ë‹¨ê³„ë¡œ ì§„í–‰
        QMessageBox.information(self, "íŒë‹¨ ê²°ê³¼", "âœ– MESSY â†’ ë¬¼ì²´ ê²€ì¶œ ì‹œì‘ \n (YOLO íƒì§€ëŠ” ì¶”í›„ ì¶”ê°€ ì˜ˆì •)")


        # ì•„ì§ ì¶”ê°€ ì•ˆë¨
        # # 2) YOLO íƒì§€ ì‹¤í–‰
        # boxes = self.detect_objects(self.current_image_path)
        #
        # # 3) íƒì§€ ê²°ê³¼ë¥¼ ì´ë¯¸ì§€ì— ê·¸ë ¤ì„œ í‘œì‹œ
        # self.draw_boxes(self.current_image_path, boxes)


if __name__ == "__main__":
    # PyQt ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    app = QApplication(sys.argv)
    window = CleanExit_AI()
    window.show()
    sys.exit(app.exec_())
